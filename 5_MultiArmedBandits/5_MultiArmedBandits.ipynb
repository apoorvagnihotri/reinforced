{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFflej4kmX5f"
   },
   "source": [
    "# Lab on Bandit Algorithms\n",
    "\n",
    "Author: Claire Vernade (claire.vernade@uni-tuebingen.de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc84U8vHmX5j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from display import plot_regret\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qId3eBfMmX5k"
   },
   "source": [
    "## Step 1 : the Bandit environment\n",
    "The bandit environment is a very simple RL environment:\n",
    "* There is only 1 state: this means actions do not imply change of state\n",
    "* There are K actions\n",
    "* The reward function is only action-dependent\n",
    "\n",
    "### Can you implement such class? Complete the reward computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffPMqQIZmX5k"
   },
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    \n",
    "    def __init__(self, means, model='gaussian', var=1.):\n",
    "        self.model = model\n",
    "        self.K = np.size(means)\n",
    "        self.means = means\n",
    "        self.var = var\n",
    "        \n",
    "    def get_reward(self, action):\n",
    "    \n",
    "        \"\"\" sample reward given action and the model of this bandit environment\"\"\"\n",
    "        if self.model == 'gaussian':\n",
    "            # TODO: add code here \n",
    "            return 1\n",
    "        else: # potentially add Bernoulli model option (not needed for exercise)\n",
    "            raise NotImplementedError('only Gaussian rewards are implemented so far')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kf4hEjkxmX5l"
   },
   "source": [
    "### Remarks / Questions:\n",
    "* The bandit environment is characterized by the family of distributions of the actions, here controlled by the keyword \"model\";\n",
    "* Building this environment, we assumed that the models admissible are parametrized by their mean *only*. Note in particular that the experimenter needs to fix the variance of the Gaussian for all arms (here with only one value, but there could be one value per arm as long as these values are known and fixed). Keep in mind that the variance is known, it is important in the construction of UCB and Thompson Sampling later. \n",
    "* Bonus: It can be useful to be able to sample binary rewards. Can you implement a new model option 'bernoulli' and the according reward sampling function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNYU1XWvmX5l"
   },
   "source": [
    "## Step 2: Bandit agents\n",
    "We have seen in class a variety of bandit algorithms, ranging from the most simple policies ($\\epsilon$-greedy,...) to the (near-)optimal UCB. But so far we only gave *theoretical* guarantees, that hold under a set of hypotheses... note also the heavy usage of big-O notations in those theorems: this means there are hidden constants everywhere!  *It is always a good scientific practice to empirically validate the theoretical claims*, so we are going to do just that :)\n",
    "\n",
    "For that purpose, we need a few more things: \n",
    "* Bandit agents: we will implement $\\epsilon$-greedy, Explore-Then-Commit and UCB,\n",
    "* We recommend that you start with $\\epsilon$-greedy and test it using Section 3 before moving on to more complex agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8kZuVPkmX5m"
   },
   "outputs": [],
   "source": [
    "class EpsGreedy():\n",
    "    \n",
    "    def __init__(self, environment, epsilon):\n",
    "        \"\"\" \n",
    "        instantiate the agent using only the available information from the environment (i.e. not the means)\n",
    "        => environment only provides K (number of available actions) here.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.K = environment.K\n",
    "        self.count_actions = np.zeros(self.K)\n",
    "        self.count_rewards = np.zeros(self.K)\n",
    "        self.t = 0\n",
    "        \n",
    "    def get_action(self):\n",
    "      #initialization\n",
    "        return 0\n",
    "\n",
    "    def receive_reward(self, action, reward):\n",
    "        # TODO: implement the update of the reward counts\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count_actions = np.zeros(self.K)\n",
    "        self.count_rewards = np.zeros(self.K)\n",
    "        self.t = 0\n",
    "        \n",
    "    def name(self):\n",
    "        return 'eps-greedy('+str(self.epsilon)+')'\n",
    "        \n",
    "\n",
    "class ETC():\n",
    "    \n",
    "    def __init__(self, environment, m):\n",
    "        \" instantiate the agent using only the available information from the environment (i.e. not the means)\"\n",
    "        self.m = m\n",
    "        self.K = environment.K\n",
    "        self.count_actions = np.zeros(self.K)\n",
    "        self.count_rewards = np.zeros(self.K)\n",
    "        self.t = 0  \n",
    "    \n",
    "    def get_action(self):\n",
    "        ## TODO: implement ETC\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "    def receive_reward(self, action, reward):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.count_actions = np.zeros(self.K)\n",
    "        self.count_rewards = np.zeros(self.K)\n",
    "        self.t = 0\n",
    "        \n",
    "    def name(self):\n",
    "        return 'ETC('+str(self.m)+')'\n",
    "    \n",
    "    \n",
    "class UCB():\n",
    "    def __init__(self, environment, var):\n",
    "        self.K = environment.K\n",
    "        self.var = var\n",
    "        self.count_actions = np.zeros(self.K)\n",
    "        self.count_rewards = np.zeros(self.K)\n",
    "        self.t = 0  \n",
    "        \n",
    "    def get_action(self):\n",
    "        ## TODO: Implement UCB\n",
    "        return 0\n",
    "        \n",
    "    def receive_reward(self, action, reward):\n",
    "        self.count_rewards[action] += reward\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count_actions = np.zeros(self.K)\n",
    "        self.count_rewards = np.zeros(self.K)\n",
    "        self.t = 0\n",
    "        \n",
    "    def name(self):\n",
    "        return 'UCB('+str(self.var)+')'        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUYBQF-ZmX5m"
   },
   "source": [
    "## Step 3: Play !\n",
    "We have an environment and a policy, we now must make them play together and collect the data so that we can observe the results.\n",
    "\n",
    "### The role of the input ``Nmc`` in the ``play`` function: \n",
    "The theorems that bound the regret are either in expectation or with high probability, so to validate them empirically, we need to observe *more than one game*. This is a very important part of these experiments: we want to build a _monte-carlo estimator of the regret_, which plays several games and allows us to observe some statistics of all trajectories. This will allow us to control that the regret does not \"explode\" once in a while, and that the *expectation* over the various sources of randomness (agent and environment) is under control. \n",
    "Long story short: the more you run trajectories, the better is your estimate of the regret and the more meaningful are your error bars :)\n",
    "\n",
    "### Building a game player, with regret loader:\n",
    "* We want to play a T-step game (horizon T) between an environment and an agent\n",
    "* We want to record the regret of our agent at every step (the agent does *not* see this!)\n",
    "* We want to be able to compare agents on a fixed environment\n",
    "\n",
    "We provide the implementation, so nothing to be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7q8pO1QmX5n"
   },
   "outputs": [],
   "source": [
    "def play(environment, agent, Nmc, T):\n",
    "    \n",
    "    data = np.zeros((Nmc, T))\n",
    "    best_reward = np.max(environment.means)\n",
    "#     print(best_reward)\n",
    "    \n",
    "    for n in range(Nmc):\n",
    "        agent.reset()\n",
    "        for t in range(T):           \n",
    "            action = agent.get_action()\n",
    "#             print('action'+str(action))\n",
    "            reward = environment.get_reward(action)\n",
    "            agent.receive_reward(action, reward)\n",
    "#             print(reward)\n",
    "            data[n,t]= best_reward - reward\n",
    "            \n",
    "    return agent.name(), data\n",
    "\n",
    "\n",
    "def experiment(environment, agents, Nmc, T):\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent_id, regrets = play(environment, agent, Nmc, T)\n",
    "        \n",
    "        all_data[agent_id] = regrets\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A plotting function\n",
    "\n",
    "Also no need to change anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regret(regrets, logscale=False, lb=None,q=10):\n",
    "    \"\"\"\n",
    "    regrets must be a dict {'agent_id':regret_table}\n",
    "    \"\"\"\n",
    "    \n",
    "    reg_plot = plt.figure()\n",
    "    #compute useful stats\n",
    "#     regret_stats = {}\n",
    "    for i, agent_id in enumerate(regrets.keys()):\n",
    "        data = regrets[agent_id]\n",
    "        N, T = data.shape\n",
    "        cumdata = np.cumsum(data, axis=1) # cumulative regret\n",
    "        \n",
    "        mean_reg = np.mean(cumdata, axis=0)\n",
    "        q_reg = np.percentile(cumdata, q, axis=0)\n",
    "        Q_reg = np.percentile(cumdata, 100-q, axis=0)\n",
    "        \n",
    "#         regret_stats[agent_id] = np.array(mean_reg, q_reg, Q_reg)\n",
    "        \n",
    "        plt.plot(np.arange(T), mean_reg, color=colors[i], label=agent_id)\n",
    "        plt.fill_between(np.arange(T), q_reg, Q_reg, color=colors[i], alpha=0.2)\n",
    "        \n",
    "    if logscale:\n",
    "        plt.xscale('log')\n",
    "        plt.xlim(left=100)\n",
    "\n",
    "    if lb is not None: \n",
    "        plt.plot(np.arange(T), lb, color='black', marker='*', markevery=int(T/10))\n",
    "        \n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('Cumulative Regret')\n",
    "    plt.legend()\n",
    "    reg_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAs0nqrlmX5n"
   },
   "source": [
    "## Observe the results and make critical observations\n",
    "\n",
    "We are now ready to run an experiment and observe the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PLgXXqkmX5o",
    "outputId": "9ee240ff-a2c8-489f-a5b4-99eb05e0e007"
   },
   "outputs": [],
   "source": [
    "means = np.array([0., 1., 2., 3.])\n",
    "basicGaussianBandit = Bandit(means)\n",
    "epsgreedy01 = EpsGreedy(basicGaussianBandit, epsilon=0.1)\n",
    "epsgreedy07 = EpsGreedy(basicGaussianBandit, epsilon=0.7)\n",
    "\n",
    "firstregrets = experiment(basicGaussianBandit, [epsgreedy01, epsgreedy07], Nmc=10, T=100)\n",
    "\n",
    "plot_regret(firstregrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq34x_3FmX5p"
   },
   "source": [
    "### Critical observations:\n",
    "* Does it seem right ? Remember that we are trying to validate theory so we should be able to check that the theorem is true. If not... two options: our code has a bug or our theorem proof has a bug :S (or hypotheses are violated).\n",
    "* What is the shape of the regret ? Do you think this policy is consistent ? Optimal ? How could you check ? \n",
    "* Which policy is the best? Is it statistically significant ?\n",
    "* Can we do better ? (see Step 5 for more)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTBNYzpemX5o"
   },
   "outputs": [],
   "source": [
    "means = np.array([0., 1., 2., 3.])\n",
    "small_var_GaussianBandit = Bandit(means, var=0.5)\n",
    "agent1 = EpsGreedy(small_var_GaussianBandit, epsilon=0.1)\n",
    "agent2 = ETC(small_var_GaussianBandit, m=10)\n",
    "\n",
    "regrets_small_var = experiment(small_var_GaussianBandit, [agent1, agent2], Nmc=20, T=100)\n",
    "plot_regret(regrets_small_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1oNOzEemX5p"
   },
   "outputs": [],
   "source": [
    "UCB_exact = UCB(basicGaussianBandit, var=0.5)\n",
    "badUCB = UCB(basicGaussianBandit, var=4.)\n",
    "lowvarUCB = UCB(basicGaussianBandit, var=0.1)\n",
    "\n",
    "regrets_UCBs = experiment(basicGaussianBandit, [UCB_exact, badUCB, lowvarUCB], Nmc=50, T=100)\n",
    "plot_regret(regrets_UCBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocZ3usCimX5p",
    "outputId": "cfc9616b-07ea-424e-b4c9-cac837b20f35"
   },
   "outputs": [],
   "source": [
    "all_agents = experiment(small_var_GaussianBandit, [agent1, agent2, UCB_exact, badUCB, lowvarUCB], Nmc=30, T=1000)\n",
    "plot_regret(all_agents, logscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbW6VuyImX5q"
   },
   "source": [
    "## Step 4: Optimality check\n",
    "\n",
    "As seen in class, there is a theoretical way to check that a policy is (close to) optimal: compare its regret with the (problem-dependent) lower bound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBQvjQ2cmX5q"
   },
   "outputs": [],
   "source": [
    "def LB(environment, T):\n",
    "    means = environment.means # the LB is NOT a policy, it is an information-theoretic quantity\n",
    "    var = environment.var\n",
    "    gaps = np.max(means) - means\n",
    "    nonzero_gaps = gaps[gaps>0.]\n",
    "    # compute the Gaussian LB:\n",
    "    lb_coeff = np.sum([2*var/gap for gap in nonzero_gaps])\n",
    "    return lb_coeff*np.log(np.arange(T)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq9V_Gl9mX5q"
   },
   "outputs": [],
   "source": [
    "LB = LB(basicGaussianBandit, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D0M3rsumX5q",
    "outputId": "1d0d5ed9-d3e5-4df9-b625-34253c5c49de"
   },
   "outputs": [],
   "source": [
    "plot_regret(all_agents, lb=LB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xawBHv5VmX5q"
   },
   "source": [
    "### Remark:\n",
    "UCB does seem close to optimal... it'd be nice to see it more clearly though. Try the log-scale option of the plot function and run the experiment on a longer horizon (the lower bound is asymptotic!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv6r2LB7mX5q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
