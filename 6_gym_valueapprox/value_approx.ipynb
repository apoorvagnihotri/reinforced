{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T16:38:02.721461Z",
     "start_time": "2023-05-23T16:38:01.969507Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "# The \"ipympl\" backend is better, as it allows for interactiveness, but on some installations it does not work, then use \"inline\"\n",
    "%matplotlib ipympl\n",
    "# %matplotlib inline  \n",
    "\n",
    "# this way of importing allow to reload/refresh the module later with importlib.reload(policy)\n",
    "import policy as policy \n",
    "import memory as mem\n",
    "from feedforward import Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment, policy and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T16:38:02.728983Z",
     "start_time": "2023-05-23T16:38:02.723428Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81, render_mode=\"human\")\n",
    "pi = policy.PDPolicy(env)\n",
    "buffer = mem.Memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T16:38:07.171082Z",
     "start_time": "2023-05-23T16:38:03.517631Z"
    }
   },
   "outputs": [],
   "source": [
    "s , _ = env.reset()\n",
    "for t in range(100):\n",
    "    a = pi.get_action(s) \n",
    "    s_new, rew, term, trunc, _ = env.step(a)\n",
    "    buffer.add_transition([s,a,rew,s_new, term, trunc])\n",
    "    s = s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T22:39:39.526595Z",
     "start_time": "2023-05-22T22:39:39.520839Z"
    }
   },
   "outputs": [],
   "source": [
    "transitions = buffer.get_all_transitions()\n",
    "states = np.vstack(transitions[:,0])\n",
    "actions = transitions[:,1]\n",
    "rewards = transitions[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the trajectory: (state, action, reward)\n",
    " \n",
    " The state is [position, velocity] \n",
    " \n",
    " **changes are needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T22:38:56.647832Z",
     "start_time": "2023-05-22T22:38:56.615286Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do 500 rollouts a 100 timesteps with the black box policy. Nothing needs to be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T22:39:44.997615Z",
     "start_time": "2023-05-22T22:39:42.304912Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g=9.81, render_mode=None) # switch off rendering\n",
    "for ep in range(500):\n",
    "    # start a larger range initial conditions to see enough of the statespace\n",
    "    s, _ = env.reset(options={\"y_init\": 4.0})\n",
    "    for t in range(100):\n",
    "        a = pi.get_action(s) \n",
    "        s_new, rew, term, trunc, _ = env.step(a)\n",
    "        buffer.add_transition([s,a,rew,s_new, term, trunc])\n",
    "        s = s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T22:39:45.003770Z",
     "start_time": "2023-05-22T22:39:44.999513Z"
    }
   },
   "outputs": [],
   "source": [
    "buffer.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T18:35:13.913704Z",
     "start_time": "2018-12-06T18:35:13.261773Z"
    }
   },
   "source": [
    "# Fit value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function fitting class. Here, you need to fill in the blank parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T12:47:34.682257Z",
     "start_time": "2020-12-15T12:47:34.671441Z"
    }
   },
   "outputs": [],
   "source": [
    "class ValueFunction(Feedforward):\n",
    "    def __init__(self, observation_dim, hidden_sizes=[100,100]):\n",
    "        super().__init__(input_size=observation_dim, hidden_sizes=hidden_sizes, output_size=1)\n",
    "        \n",
    "        # so this class is already a neural network, see feedforward.py        \n",
    "        # add and optimizer here (e.g. Adam with lr = 0.0002, eps=0.000001)\n",
    "        self.optimizer = None\n",
    "        \n",
    "        # add a loss function here\n",
    "        self.loss = None\n",
    "    \n",
    "    def fit(self, observations, targets):\n",
    "        # Add one training step here. \n",
    "        # The states are called here observations        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Hints:         \n",
    "        # convert the inputs into torch tensors with torch.from_numpy\n",
    "        # use self.forward(input) to make a prediction to be used in the loss\n",
    "        \n",
    "        # return the current loss\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T12:47:35.137367Z",
     "start_time": "2020-12-15T12:47:35.131626Z"
    }
   },
   "outputs": [],
   "source": [
    "valuefunc = ValueFunction(observation_dim=env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting routine of the value function. Fill in the blank parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:50:55.906325Z",
     "start_time": "2020-12-15T14:50:55.902084Z"
    }
   },
   "outputs": [],
   "source": [
    "plt_fit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:53:25.341257Z",
     "start_time": "2020-12-15T14:53:09.713410Z"
    }
   },
   "outputs": [],
   "source": [
    "iter_fit = 100 # do at least 2000\n",
    "gamma=0.95\n",
    "for i in range(iter_fit):\n",
    "\n",
    "    # sample from the replay buffer\n",
    "    data=buffer.sample(batch=512)\n",
    "    # each entry in data is (state,action,reward,next_state, term, trunc)    \n",
    "    \n",
    "    # Hints:\n",
    "    #  use can use np.stack to convert the array or arrays into one array\n",
    "    #  it is really important that you check the shapes of your arrays.\n",
    "    #  It should be (128,2) for the state and (128,1) for reward and values\n",
    "    \n",
    "    state = None\n",
    "    \n",
    "    td_target = None \n",
    "    \n",
    "    # optimize the least squared objective\n",
    "    fit_loss = valuefunc.fit(states, td_target)\n",
    "    \n",
    "    plt_fit.append(fit_loss)\n",
    "    \n",
    "# plot the loss\n",
    "plt.figure()\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(range(len(plt_fit)),plt_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T07:37:06.258855Z",
     "start_time": "2018-12-07T07:37:06.255274Z"
    }
   },
   "source": [
    "Visualization of the value function. Nothing needs to be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:53:30.804526Z",
     "start_time": "2020-12-15T14:53:30.714375Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_value_function(value_function):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    xxs =np.linspace(-np.pi/2,np.pi/2)\n",
    "    yys =np.linspace(-3,3)\n",
    "    XX,YY=np.meshgrid(xxs,yys)\n",
    "    dots=np.asarray([np.cos(XX.ravel()),np.sin(XX.ravel()),YY.ravel()]).T\n",
    "    print(dots.shape)\n",
    "    # values = np.asarray(test_func(dots)).reshape(XX.shape)\n",
    "    values =value_function.predict(dots).reshape(XX.shape)\n",
    "\n",
    "    fig = plt.figure(figsize=[10,8])\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    surf = ax.plot_surface(XX, YY, values, cmap=cm.coolwarm,\n",
    "                           linewidth=0, antialiased=False)\n",
    "    ax.view_init(elev=30, azim=45, roll=0)\n",
    "    ax.set_xlabel('angle')\n",
    "    ax.set_ylabel('angle velocity')\n",
    "    ax.set_zlabel('value')\n",
    "    # plt.colorbar(cmap=cm.coolwarm)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"value_approx.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gym-RL",
   "language": "python",
   "name": "gym-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
