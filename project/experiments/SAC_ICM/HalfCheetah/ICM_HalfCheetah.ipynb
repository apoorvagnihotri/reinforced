{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475617c6-caad-41f4-8e5d-163c729ef869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import collections, random\n",
    "\n",
    "# Device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af271d-aeb3-454a-a9b9-6d490119a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc38f30-d15d-48ca-8dc5-647cb2d5d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICM(nn.Module):\n",
    "    def __init__(self, state_size, action_size, icm_parameters):\n",
    "        super(ICM, self).__init__()\n",
    "\n",
    "        feature_hidden_sizes, feature_size, inverse_hidden_sizes, forward_hidden_sizes, self.β, icm_lr = icm_parameters\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Feature NN\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_size, feature_hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_hidden_sizes[0], feature_hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_hidden_sizes[1], feature_size)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Inverse NN\n",
    "        self.inverse_net = nn.Sequential(\n",
    "            nn.Linear(feature_size * 2, inverse_hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inverse_hidden_sizes[0], inverse_hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inverse_hidden_sizes[1], action_size)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Forward NN\n",
    "        self.forward_net = nn.Sequential(\n",
    "            nn.Linear(feature_size + action_size, forward_hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_hidden_sizes[0], forward_hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_hidden_sizes[1], feature_size)\n",
    "        ).to(device)\n",
    "\n",
    "        self.L_I = nn.MSELoss()\n",
    "        self.L_F = nn.MSELoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=icm_lr)\n",
    "        \n",
    "    def predict(self, s, a, s_prime):\n",
    "        '''Predicts next action and state.'''\n",
    "        s_reshaped = s.view(-1,self.state_size)\n",
    "        a_reshaped = a.view(-1,self.action_size)\n",
    "        s_prime_reshaped = s_prime.view(-1,self.state_size)\n",
    "        cat1 = torch.cat([self.feature_net(s_reshaped), self.feature_net(s_prime_reshaped)], 1)\n",
    "        cat2 = torch.cat([self.feature_net(s_reshaped).clone().detach(), a_reshaped], 1)\n",
    "        a_hat = self.inverse_net(cat1)\n",
    "        s_hat = self.forward_net(cat2)\n",
    "        return a_hat, s_hat\n",
    "\n",
    "    def predict_once(self, s, a, s_prime):\n",
    "        '''Predicts next action and state for a single observation.'''\n",
    "        s = torch.from_numpy(s).float()\n",
    "        s_prime = torch.from_numpy(s_prime).float()\n",
    "        cat1 = torch.cat([self.feature_net(s), self.feature_net(s_prime)])\n",
    "        cat2 = torch.cat([self.feature_net(s).clone().detach(), a])\n",
    "        a_hat = self.inverse_net(cat1)\n",
    "        s_hat = self.forward_net(cat2)\n",
    "        return a_hat, s_hat\n",
    "\n",
    "    def loss(self, s, a, a_hat, s_prime, s_hat):\n",
    "        '''Calculates loss.'''\n",
    "        L_I = self.L_I(a_hat, a.view(-1,self.action_size))\n",
    "        L_F = self.L_F(s_hat, self.feature_net(s_prime.view(-1,self.state_size)).clone().detach())\n",
    "        return (1 - self.β) * L_I + self.β * L_F\n",
    "\n",
    "    def loss_once(self, s, a, a_hat, s_prime, s_hat):\n",
    "        '''Calculates loss for a single prediction.'''\n",
    "        s_prime = torch.from_numpy(s_prime).float()\n",
    "        L_I = self.L_I(a_hat, a)\n",
    "        L_F = self.L_F(s_hat, self.feature_net(s_prime).clone().detach())\n",
    "        return (1 - self.β) * L_I + self.β * L_F\n",
    "\n",
    "    def compute_intrinsic_reward(self, s, a, s_prime):\n",
    "        \"\"\"\n",
    "        Computes intrinsic reward based on ICM forward model error.\n",
    "\n",
    "        \"\"\"\n",
    "        # Forward model prediction\n",
    "        _, s_hat = self.predict(s, a, s_prime)\n",
    "        \n",
    "        # Intrinsic reward is the forward model error\n",
    "        intrinsic_reward = F.mse_loss(s_hat, self.feature_net(s_prime).clone().detach(), reduction='none')\n",
    "        #print(f'intrinsic_reward.shape: {intrinsic_reward.shape}') #intrinsic_reward.shape : (32,16->latent_dim)\n",
    "        return intrinsic_reward.sum(dim=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd3c3c-8255-4e18-bcb6-d8cdb00492ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Soft Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c6819-9009-435f-ba1f-26c8aa0a7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "lr_pi           = 0.0005\n",
    "lr_q            = 0.001\n",
    "init_alpha      = 0.01\n",
    "gamma           = 0.98\n",
    "batch_size      = 128\n",
    "buffer_limit    = 50000\n",
    "tau             = 0.01 # for target network soft update\n",
    "target_entropy  = -1.0 # for automated alpha update\n",
    "lr_alpha        = 0.001  # for automated alpha update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56146a64-c2c9-4ca4-afe4-f947fb79fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        #print(type(s_lst))\n",
    "        #print(torch.tensor(a_lst).shape)\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n",
    "                torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fd6ea-beb1-4c2b-b2be-7d29ee54102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, learning_rate, icm_params, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128).to(device)\n",
    "        self.fc_mu = nn.Linear(128,action_dim).to(device)\n",
    "        self.fc_std  = nn.Linear(128,action_dim).to(device)\n",
    "        \n",
    "        \n",
    "        #self.icm = icm = ICM(state_size=state_dim, action_size=action_dim, icm_parameters=icm_params)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7)\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_net(self, q1, q2, mini_batch, intrinsic_reward):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob #Entropy loss term, weighted by alpha\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        #print(f'q1_q2 cat shape: {q1_q2.shape}')\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        '''\n",
    "        Combined loss used before:\n",
    "        # Update ICM module\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, _ = mini_batch\n",
    "        a_hat_batch, s_hat_batch = self.icm.predict(s_batch.to(device), a_batch.to(device), s_prime_batch.to(device))\n",
    "        intrinsic_loss = self.icm.loss(s_batch, a_batch, a_hat_batch, s_prime_batch, s_hat_batch)\n",
    "\n",
    "        extrinsic_loss = -min_q - entropy # for gradient ascent\n",
    "        \n",
    "        \n",
    "        print(f\"Extrinsic Loss:{extrinsic_loss.mean()}, Intrinsic Loss:{intrinsic_loss.mean()}\")\n",
    "        \n",
    "        #Combinng the losses:\n",
    "        loss = extrinsic_loss + lambda_intrinsic*intrinsic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "        '''\n",
    "\n",
    "        intrinsic_reward = intrinsic_reward.clone().detach()\n",
    "        #print(f'min_q shape: {min_q.shape}, entropy.shape: {entropy.shape}')  #min_q shape : (32,1)    entropy.shape : (32,6 -> action_dim), \n",
    "        loss = -min_q - entropy +  lambda_intrinsic*intrinsic_reward.mean()# for gradient ascent (CHECK IF ADDING REWARD LIKE THIS IS OKAY OR NOT)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        return -min_q - entropy\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af5650-5843-48f8-9353-79f22120df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dim, action_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc_s = nn.Linear(state_dim, 64).to(device)\n",
    "        self.fc_a = nn.Linear(action_dim,64).to(device)\n",
    "        self.fc_cat = nn.Linear(128,32).to(device)\n",
    "        self.fc_out = nn.Linear(32,action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = x.to(device)\n",
    "        a = a.to(device)\n",
    "        h1 = F.relu(self.fc_s(x))\n",
    "        h2 = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([h1,h2], dim=1)\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "    def train_net(self, target, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s, a) , target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b6b1e-f2f3-4424-a0ae-95d7c30a822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_target(pi, q1, q2, mini_batch):\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "        #print(r.device, done.device, min_q.device, entropy.device)\n",
    "        target = r + gamma * done * (min_q + entropy)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c9b1f-6b56-46ec-a74d-c61339175907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b96c5c-40e7-455f-a5c2-789573c989a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f52abcc-cfad-4c49-9880-2bb024aa1631",
   "metadata": {},
   "source": [
    "## ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e14a3-003d-4610-a972-68abef937167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAC Hyperparameters\n",
    "lr_pi           = 0.0005\n",
    "lr_q            = 0.001\n",
    "init_alpha      = 0.01\n",
    "gamma           = 0.98\n",
    "batch_size      = 256\n",
    "buffer_limit    = 50000\n",
    "tau             = 0.01 # for target network soft update\n",
    "target_entropy  = -6.0 # for automated alpha update\n",
    "lr_alpha        = 0.001  # for automated alpha update\n",
    "\n",
    "\n",
    "# ICM parameters\n",
    "feature_hidden_sizes = [128,128,]\n",
    "feature_size = 16\n",
    "inverse_hidden_sizes = [128,128,]\n",
    "forward_hidden_sizes = [128,128,]\n",
    "β = 0.5\n",
    "icm_lr = 0.001\n",
    "lambda_intrinsic = 0.01 #controls trade-off between extrinsic loss and intrinsic loss2.0, 2.0, 10.0, 10.0, 4,0 ,4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70878c-1469-4103-82fa-a2a3d9094928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('HalfCheetah-v4', render_mode=\"rgb_array\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "        \n",
    "    \n",
    "    memory = ReplayBuffer()\n",
    "    q1, q2, q1_target, q2_target = QNet(lr_q, state_dim, action_dim).to(device), QNet(lr_q, state_dim, action_dim).to(device), QNet(lr_q, state_dim, action_dim).to(device), QNet(lr_q, state_dim, action_dim).to(device)\n",
    "    icm_params = (feature_hidden_sizes, feature_size, inverse_hidden_sizes, forward_hidden_sizes, β, icm_lr)\n",
    "    pi = PolicyNet(lr_pi, icm_params, state_dim, action_dim).to(device)\n",
    "    icm = ICM(state_size=state_dim, action_size=action_dim, icm_parameters=icm_params).to(device)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    icm_params = (feature_hidden_sizes, feature_size, inverse_hidden_sizes, forward_hidden_sizes, β, icm_lr)\n",
    "    icm = ICM(state_size=state_dim, action_size=action_dim, icm_parameters=icm_params)'''\n",
    "\n",
    "    q1_target.load_state_dict(q1.state_dict())\n",
    "    q2_target.load_state_dict(q2.state_dict())\n",
    "\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        \n",
    "        # Only render every 100 episodes\n",
    "        render_env = (n_epi % 100 == 0)\n",
    "\n",
    "        while count < 1000 and not done:\n",
    "            \n",
    "            #if render_env:\n",
    "                #env.render()  # Render the environment\n",
    "            \n",
    "            \n",
    "            a, log_prob= pi(torch.from_numpy(s).float())\n",
    "            a = a.cpu().detach().numpy()\n",
    "            s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "            memory.put((s, a, r, s_prime, done))  #Changed to r from r/10.0\n",
    "            score +=r\n",
    "            s = s_prime\n",
    "            count += 1\n",
    "\n",
    "        icm_losses = []\n",
    "        sac_losses = []\n",
    "        if memory.size()>10000:\n",
    "            for i in range(100):\n",
    "                mini_batch = memory.sample(batch_size)\n",
    "                mini_batch = tuple(t.to(device) for t in mini_batch)\n",
    "                \n",
    "                td_target = calc_target(pi, q1_target, q2_target, mini_batch)\n",
    "                \n",
    "                \n",
    "                q1.train_net(td_target, mini_batch)\n",
    "                q2.train_net(td_target, mini_batch)\n",
    "\n",
    "                # Update ICM module\n",
    "                s_batch, a_batch, r_batch, s_prime_batch, _ = mini_batch\n",
    "                intrinsic_reward = icm.compute_intrinsic_reward(s_batch, a_batch, s_prime_batch)\n",
    "                a_hat_batch, s_hat_batch = icm.predict(s_batch, a_batch, s_prime_batch)\n",
    "                intrinsic_loss = icm.loss(s_batch, a_batch, a_hat_batch, s_prime_batch, s_hat_batch) #intrinsic_loss.shape : [] -> rank 0 tensor\n",
    "                icm_losses.append(intrinsic_loss.item())\n",
    "                \n",
    "                icm.optimizer.zero_grad()\n",
    "                intrinsic_loss.backward()\n",
    "                icm.optimizer.step()\n",
    "                \n",
    "                #Update policy net\n",
    "                sac_loss = pi.train_net(q1, q2, mini_batch, intrinsic_reward).mean().item()  #sac_loss.shape : [32,6] \n",
    "                #print(sac_loss.shape)\n",
    "                sac_losses.append(sac_loss)\n",
    "                \n",
    "                \n",
    "                q1.soft_update(q1_target)\n",
    "                q2.soft_update(q2_target)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f} alpha:{:.4f}\".format(n_epi, score/print_interval, pi.log_alpha.exp()))\n",
    "            print(f'Avg ICM loss: {np.mean(icm_losses)}')\n",
    "            print(f'Avg SAC loss: {np.mean(sac_losses)}')\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bead0-348d-48d2-9fed-3d2b00f5b932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d8f81-3b85-49b8-b856-a6152bfe9ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd157a7f-c89b-446c-babc-6fa1a8195c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e51868-1b64-4164-8dc2-b9ef5c7a9f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef175273-537a-4aa5-baed-486c942beb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ba16b-6950-4c79-a170-91a082eae91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
